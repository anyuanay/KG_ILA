{"cells":[{"cell_type":"markdown","metadata":{"id":"kN5nPr4m6SiL"},"source":["# Classify Voice Clips on Combination of the following Features using Deep Neural Networks:\n","In this notebook, we will build classifiers to classify voice clips on the features that have been extracted. The following features are extracted:\n","1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n","2. Chroma STFT (Short-Time Fourier Transform): Refers to the chroma feature representation derived from the short-time Fourier transform of an audio signal. Chroma features, or chromagrams, represent the energy distribution among the twelve different pitch classes (C, C#, D, ..., B) of the musical octave. .\n","3. Mel Spectrogram: A Mel spectrogram is a representation of the power spectrum of a sound signal, where the frequencies are converted to the Mel scale. The Mel scale is designed to mimic the human ear's perception of sound, where each Mel unit corresponds to a perceived equal step in pitch.\n","4.  MFCC: Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.\n","5. RMS: root-mean-square (RMS) value for each frame, either from the audio samples or from a spectrogram.\n","6. Chroma CQT: Constant-Q chromagram\n","7. Chroma CENS: Chroma variant “Chroma Energy Normalized” (CENS)\n","8. Chroma VQT: Variable-Q chromagram\n","9. Spectral Centroid : The center of gravity of the spectrum.\n","10. Spectral Bandwidth: Compute pth-order spectral bandwidth..\n","11. Spectral Contrast :  Compute spectral contrast.\n","12. Spectral Flatness: Compute spectral flatness\n","13. Spectral Rolloff : The roll-off frequency is defined for each frame as the center frequency for a spectrogram bin such that at least roll_percent (0.85 by default) of the energy of the spectrum in this frame is contained in this bin and the bins below. This can be used to, e.g., approximate the maximum (or minimum) frequency by setting roll_percent to a value close to 1 (or 0)."]},{"cell_type":"markdown","metadata":{"id":"BZ6FEwVR7VqR"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BOQ6WMN7Zs1"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"KwAMXusO7C3y"},"source":["## Load the Data and Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIrx1aMRMC15"},"outputs":[],"source":["data_features_path = A_PATH_TO_DATA_FEATURES\n","\n","test_data_features_path = A_PATH_TO_TEST_DATA_FEATURES\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gY-f4f9sMC15"},"outputs":[],"source":["all_data_features = pd.read_csv(data_features_path)\n","\n","test_data_features = pd.read_csv(test_data_features_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCfpeA3SMC15"},"outputs":[],"source":["all_data_features.shape, test_data_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9djJ5_IMC16"},"outputs":[],"source":["all_data_features.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZgCdwYUMC16"},"outputs":[],"source":["from itertools import combinations\n","\n","def generate_subsets_min_length(lst, min_length):\n","    subsets = []\n","    for i in range(min_length, len(lst) + 1):\n","        subsets.extend(combinations(lst, i))\n","    return [list(subset) for subset in subsets]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZNQd9mtMC16"},"outputs":[],"source":["features_list = ['zcrate_mean',\n","       'chroma_stft_mean', 'melspectrogram_mean', 'mfcc_feature', 'rms_mean',\n","       'chroma_cqt_mean', 'chroma_cens_mean', 'chroma_vqt_mean', 'spcent_mean',\n","       'spband_mean', 'spcontrast_mean', 'spflat_mean', 'sprolloff_mean']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QobjG3pFMC16"},"outputs":[],"source":["len(features_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rflPWftkMC16"},"outputs":[],"source":["all_features_combs = generate_subsets_min_length(features_list, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq_nZOBzMC17"},"outputs":[],"source":["len(all_features_combs)"]},{"cell_type":"markdown","metadata":{"id":"3OKCgQCYErkm"},"source":["## Evaluat on Different Sets of Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZKl39OzP0vi"},"outputs":[],"source":["all_data_features.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV-7WJAbUQgd"},"outputs":[],"source":["import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOehFo25UQge"},"outputs":[],"source":["from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpuLkBD4UQge"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_KOfLOiUQge"},"outputs":[],"source":["def create_cnn_model(n_features):\n","\n","    model = Sequential()\n","    # 1st Convolutional Layer\n","    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_features, 1)))\n","    model.add(MaxPooling1D(pool_size=2))\n","\n","    # 2nd Convolutional Layer\n","    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","\n","    # 3rd Convolutional Layer\n","    #model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","    #model.add(MaxPooling1D(pool_size=2))\n","\n","    # Flatten the output of the last pooling layer\n","    model.add(Flatten())\n","\n","    # Fully connected layer\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.5))\n","\n","    # Output layer\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VATTOcwxUQge"},"outputs":[],"source":["def create_denseLayer_model(n_features):\n","\n","    model = Sequential()\n","    model.add(Input(shape=(n_features,)))\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dense(16, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMDt8ZrWUQge"},"outputs":[],"source":["import ast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91YayGnKUQgf"},"outputs":[],"source":["features_combs = generate_subsets_min_length(features_list, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O2pMyEBUQgk"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import tqdm\n","import ast\n","import time"]},{"cell_type":"markdown","metadata":{"id":"q4mS_6-DUQgk"},"source":["## NN on 1 Feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ebRoc1TUQgk"},"outputs":[],"source":["features_list_list = [[feature] for feature in features_list]\n","features_list_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7knkTH282L6L","scrolled":true},"outputs":[],"source":["%%time\n","\n","records = []\n","\n","for features_comb in tqdm(features_list_list):\n","\n","    columns_to_use = ['label']\n","    columns_to_use.extend(features_comb)\n","\n","    data = all_data_features[columns_to_use]\n","\n","    test_data = test_data_features[columns_to_use]\n","\n","    data = data.copy()\n","\n","    col_to_convert = []\n","    for acol in features_comb:\n","        if data[acol].dtype == 'object':\n","            col_to_convert.append(acol)\n","\n","    # Create a list to hold the new DataFrames\n","    new_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        data[col] = data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_col_df = pd.DataFrame(data[col].tolist(), index=data.index)\n","        new_col_df.columns = [f\"{col}_{idx}\" for idx in new_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_cols_df.append(new_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_data = pd.concat([data] + new_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_data = new_data.drop(columns=col_to_convert)\n","\n","    ### Do the same for test data\n","\n","    test_data = test_data.copy()\n","\n","    # Create a list to hold the new DataFrames\n","    new_test_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        test_data[col] = test_data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_test_col_df = pd.DataFrame(test_data[col].tolist(), index=test_data.index)\n","        new_test_col_df.columns = [f\"{col}_{idx}\" for idx in new_test_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_test_cols_df.append(new_test_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_test_data = pd.concat([test_data] + new_test_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_test_data = new_test_data.drop(columns=col_to_convert)\n","\n","\n","    ## make the train, test, and validation data sets\n","\n","    X = new_data.drop(columns=['label']).values\n","\n","    X_val = new_test_data.drop(columns=['label']).values\n","\n","    y = np.array(new_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    y_val = np.array(new_test_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # make train as the whole set\n","    X_train = X\n","    y_train = y\n","\n","    # scale the features\n","    sc = StandardScaler()\n","    sc.fit(X_train)\n","\n","    X_train_std = sc.transform(X_train)\n","    X_test_std = sc.transform(X_test)\n","    X_val_std = sc.transform(X_val)\n","\n","    ## Classify by NN\n","\n","    # use the n_features to switch between cnn and dense layers\n","    n_features = X_train_std.shape[1]\n","\n","    model_name = \"NN\"\n","\n","    try:\n","        if n_features < 20:\n","\n","            #print(\"Use Dense Layers: {}\".format(n_features))\n","            model = create_denseLayer_model(n_features)\n","\n","            model_name = \"Dense Layers\"\n","\n","        else:\n","            #print(\"Use CNN: {}\".format(n_features))\n","            model = create_cnn_model(n_features)\n","\n","            # make the right shape of the inputs\n","            X_train_std = X_train_std[..., None]  # Add a channel dimension\n","            X_test_std = X_test_std[..., None]    # Add a channel dimension\n","\n","            X_val_std = X_val_std[..., None]      # Add a channel dimension\n","\n","            model_name = \"CNN\"\n","\n","\n","        # Train the model\n","        model.fit(X_train_std, y_train, epochs=20, batch_size=32, validation_data=(X_test_std, y_test), verbose=0)\n","\n","        # Make predictions\n","        y_pred_prob = model.predict(X_test_std)\n","        y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary outputs\n","\n","        rec = {}\n","        rec['features'] = features_comb\n","        rec['model'] = model_name\n","        rec['evaluation_data'] = 'split from training'\n","        rec['accuracy'] = accuracy_score(y_test, y_pred)\n","        rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","        rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","        rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","        rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","        rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","        rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","        records.append(rec)\n","\n","        # Make predictions on individual set\n","        y_val_pred_prob = model.predict(X_val_std)\n","        y_val_pred = (y_val_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary outputs\n","\n","        rec = {}\n","        rec['features'] = features_comb\n","        rec['model'] = model_name\n","        rec['evaluation_data'] = 'individual set'\n","        rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","        rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","        rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","        rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","        rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","        rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","        rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","        records.append(rec)\n","\n","        if len(records) % 40 == 0:\n","            evals = pd.DataFrame(records)\n","            evals.to_csv(\"test_evaluation_nn_1_feature_results.csv\", index=None)\n","    except Exception as e:\n","        print(\"There are issues for features: {}\".format(features_comb))\n","\n","evals = pd.DataFrame(records)\n","evals.to_csv(\"test_evaluation_nn_1_feature_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvjC0bxjUQgl"},"outputs":[],"source":["evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga-8rQ1sKkNt"},"outputs":[],"source":["f1_boring_max = evals[evals.evaluation_data == 'individual set'].f1_boring.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnzCCnaDMC18"},"outputs":[],"source":["evals[evals.f1_boring == f1_boring_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr94AKaPMC18"},"outputs":[],"source":["evals[evals.evaluation_data == 'individual set'].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Z5rSfnwUQgl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dqZunFqUQgl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1zIRN-WsUQgl"},"source":["## NN on At Least 2 Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8Z4PoycUQgl"},"outputs":[],"source":["len(features_combs[4400:])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Gg_Ph-i5UQgl"},"outputs":[],"source":["%%time\n","\n","records = []\n","\n","for features_comb in tqdm(features_combs[4400:]):\n","\n","    columns_to_use = ['label']\n","    columns_to_use.extend(features_comb)\n","\n","    data = all_data_features[columns_to_use]\n","\n","    test_data = test_data_features[columns_to_use]\n","\n","    data = data.copy()\n","\n","    col_to_convert = []\n","    for acol in features_comb:\n","        if data[acol].dtype == 'object':\n","            col_to_convert.append(acol)\n","\n","    # Create a list to hold the new DataFrames\n","    new_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        data[col] = data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_col_df = pd.DataFrame(data[col].tolist(), index=data.index)\n","        new_col_df.columns = [f\"{col}_{idx}\" for idx in new_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_cols_df.append(new_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_data = pd.concat([data] + new_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_data = new_data.drop(columns=col_to_convert)\n","\n","    ### Do the same for test data\n","\n","    test_data = test_data.copy()\n","\n","    # Create a list to hold the new DataFrames\n","    new_test_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        test_data[col] = test_data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_test_col_df = pd.DataFrame(test_data[col].tolist(), index=test_data.index)\n","        new_test_col_df.columns = [f\"{col}_{idx}\" for idx in new_test_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_test_cols_df.append(new_test_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_test_data = pd.concat([test_data] + new_test_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_test_data = new_test_data.drop(columns=col_to_convert)\n","\n","\n","    ## make the train, test, and validation data sets\n","\n","    X = new_data.drop(columns=['label']).values\n","\n","    X_val = new_test_data.drop(columns=['label']).values\n","\n","    y = np.array(new_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    y_val = np.array(new_test_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # make train as the whole set\n","    X_train = X\n","    y_train = y\n","\n","    # scale the features\n","    sc = StandardScaler()\n","    sc.fit(X_train)\n","\n","    X_train_std = sc.transform(X_train)\n","    X_test_std = sc.transform(X_test)\n","    X_val_std = sc.transform(X_val)\n","\n","    ## Classify by NN\n","\n","    # use the n_features to switch between cnn and dense layers\n","    n_features = X_train_std.shape[1]\n","\n","    model_name = \"NN\"\n","\n","    try:\n","        if n_features < 20:\n","\n","            #print(\"Use Dense Layers: {}\".format(n_features))\n","            model = create_denseLayer_model(n_features)\n","\n","            model_name = \"Dense Layers\"\n","\n","        else:\n","            #print(\"Use CNN: {}\".format(n_features))\n","            model = create_cnn_model(n_features)\n","\n","            # make the right shape of the inputs\n","            X_train_std = X_train_std[..., None]  # Add a channel dimension\n","            X_test_std = X_test_std[..., None]    # Add a channel dimension\n","\n","            X_val_std = X_val_std[..., None]      # Add a channel dimension\n","\n","            model_name = \"CNN\"\n","\n","\n","        # Train the model\n","        model.fit(X_train_std, y_train, epochs=20, batch_size=32, validation_data=(X_test_std, y_test), verbose=0)\n","\n","        # Make predictions\n","        y_pred_prob = model.predict(X_test_std)\n","        y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary outputs\n","\n","        rec = {}\n","        rec['features'] = features_comb\n","        rec['model'] = model_name\n","        rec['evaluation_data'] = 'split from training'\n","        rec['accuracy'] = accuracy_score(y_test, y_pred)\n","        rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","        rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","        rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","        rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","        rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","        rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","        records.append(rec)\n","\n","        # Make predictions on individual set\n","        y_val_pred_prob = model.predict(X_val_std)\n","        y_val_pred = (y_val_pred_prob > 0.5).astype(\"int32\")  # Convert probabilities to binary outputs\n","\n","        rec = {}\n","        rec['features'] = features_comb\n","        rec['model'] = model_name\n","        rec['evaluation_data'] = 'individual set'\n","        rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","        rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","        rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","        rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","        rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","        rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","        rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","        records.append(rec)\n","\n","        if len(records) % 40 == 0:\n","            evals = pd.DataFrame(records)\n","            evals.to_csv(\"test_evaluation_nn_4400_results.csv\", index=None)\n","    except Exception as e:\n","        print(\"There are issues for features: {}\".format(features_comb))\n","\n","evals = pd.DataFrame(records)\n","evals.to_csv(\"test_evaluation_nn_4400_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1UWIpdZUQgm"},"outputs":[],"source":["evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMitNxKsUQgm"},"outputs":[],"source":["f1_boring_max = evals[evals.evaluation_data == 'individual set'].f1_boring.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DurSDNWbUQgm"},"outputs":[],"source":["evals[evals.f1_boring == f1_boring_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgp1MNJmUQgn"},"outputs":[],"source":["evals[evals.evaluation_data == 'individual set'].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"B29jj0owMC18"},"source":["## Merge all results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftfY-G8wUQgn"},"outputs":[],"source":["results_1 = pd.read_csv(\"test_evaluation_nn_1_results.csv\")\n","results_2 = pd.read_csv(\"test_evaluation_nn_760_results.csv\")\n","results_3 = pd.read_csv(\"test_evaluation_nn_4400_results.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgsZkLYHUQgn"},"outputs":[],"source":["results_1.shape, results_2.shape, results_3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZCAOG6xUQgn"},"outputs":[],"source":["results = pd.concat([results_1, results_2, results_3], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"267MDc0HUQgn"},"outputs":[],"source":["results.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCk6RdfaUQgn"},"outputs":[],"source":["results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGSeUudRUQgn"},"outputs":[],"source":["#results.to_csv(\"test_evaluation_nn_all_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lABCYPvUUQgn"},"outputs":[],"source":["f1_boring_max = results[results.evaluation_data == 'individual set'].f1_boring.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHGHsOIlUQgn"},"outputs":[],"source":["max_boring_f1 = results[results.f1_boring == f1_boring_max]\n","max_boring_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tI_-dRdUQgn"},"outputs":[],"source":["max_boring_f1.features.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xw6hWWZPUQgn"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"drtutor","language":"python","name":"drtutor"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}