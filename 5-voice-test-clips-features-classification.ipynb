{"cells":[{"cell_type":"markdown","metadata":{"id":"kN5nPr4m6SiL"},"source":["# Classify Voice Clips on Features\n","### In this notebook, we will build classifiers to classify voice clips on the features that have been extracted. The following features are extracted:\n","1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n","2. Chroma STFT (Short-Time Fourier Transform): Refers to the chroma feature representation derived from the short-time Fourier transform of an audio signal. Chroma features, or chromagrams, represent the energy distribution among the twelve different pitch classes (C, C#, D, ..., B) of the musical octave. .\n","3. Mel Spectrogram: A Mel spectrogram is a representation of the power spectrum of a sound signal, where the frequencies are converted to the Mel scale. The Mel scale is designed to mimic the human ear's perception of sound, where each Mel unit corresponds to a perceived equal step in pitch.\n","4.  MFCC: Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.\n","5. RMS: root-mean-square (RMS) value for each frame, either from the audio samples or from a spectrogram.\n","6. Chroma CQT: Constant-Q chromagram\n","7. Chroma CENS: Chroma variant “Chroma Energy Normalized” (CENS)\n","8. Chroma VQT: Variable-Q chromagram\n","9. Spectral Centroid : The center of gravity of the spectrum.\n","10. Spectral Bandwidth: Compute pth-order spectral bandwidth..\n","11. Spectral Contrast :  Compute spectral contrast.\n","12. Spectral Flatness: Compute spectral flatness\n","13. Spectral Rolloff : The roll-off frequency is defined for each frame as the center frequency for a spectrogram bin such that at least roll_percent (0.85 by default) of the energy of the spectrum in this frame is contained in this bin and the bins below. This can be used to, e.g., approximate the maximum (or minimum) frequency by setting roll_percent to a value close to 1 (or 0)."]},{"cell_type":"markdown","metadata":{"id":"BZ6FEwVR7VqR"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BOQ6WMN7Zs1"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"KwAMXusO7C3y"},"source":["## Load the Data and Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIrx1aMRMC15"},"outputs":[],"source":["data_features_path = A_PATH\n","\n","test_data_features_path = A_PATH\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gY-f4f9sMC15"},"outputs":[],"source":["all_data_features = pd.read_csv(data_features_path)\n","\n","test_data_features = pd.read_csv(test_data_features_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCfpeA3SMC15"},"outputs":[],"source":["all_data_features.shape, test_data_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9djJ5_IMC16"},"outputs":[],"source":["all_data_features.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZgCdwYUMC16"},"outputs":[],"source":["from itertools import combinations\n","\n","def generate_subsets_min_length(lst, min_length):\n","    subsets = []\n","    for i in range(min_length, len(lst) + 1):\n","        subsets.extend(combinations(lst, i))\n","    return [list(subset) for subset in subsets]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZNQd9mtMC16"},"outputs":[],"source":["features_list = ['zcrate_mean',\n","       'chroma_stft_mean', 'melspectrogram_mean', 'mfcc_feature', 'rms_mean',\n","       'chroma_cqt_mean', 'chroma_cens_mean', 'chroma_vqt_mean', 'spcent_mean',\n","       'spband_mean', 'spcontrast_mean', 'spflat_mean', 'sprolloff_mean']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QobjG3pFMC16"},"outputs":[],"source":["len(features_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rflPWftkMC16"},"outputs":[],"source":["all_features_combs = generate_subsets_min_length(features_list, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq_nZOBzMC17"},"outputs":[],"source":["len(all_features_combs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQpnLGtRTzC3"},"outputs":[],"source":["all_features_combs = generate_subsets_min_length(features_list, 1)\n","len(all_features_combs)"]},{"cell_type":"markdown","metadata":{"id":"3OKCgQCYErkm"},"source":["## Test on Different Sets of Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZKl39OzP0vi"},"outputs":[],"source":["all_data_features.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YktAcuqVMC17"},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"markdown","metadata":{"id":"8Y1xOVhwTzC3"},"source":["## With 1 Feature and Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lkFEqSfTzC4"},"outputs":[],"source":["features_list_list = [[feature] for feature in features_list]\n","features_list_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7knkTH282L6L"},"outputs":[],"source":["## on various feature combinations\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import tqdm\n","import ast\n","\n","records = []\n","\n","features_combs = features_list_list\n","\n","for features_comb in tqdm(features_combs):\n","\n","    columns_to_use = ['label']\n","    columns_to_use.extend(features_comb)\n","\n","    data = all_data_features[columns_to_use]\n","\n","    test_data = test_data_features[columns_to_use]\n","\n","\n","    data = data.copy()\n","\n","    col_to_convert = []\n","    for acol in features_comb:\n","        if data[acol].dtype == 'object':\n","            col_to_convert.append(acol)\n","\n","    # Create a list to hold the new DataFrames\n","    new_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        data[col] = data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_col_df = pd.DataFrame(data[col].tolist(), index=data.index)\n","        new_col_df.columns = [f\"{col}_{idx}\" for idx in new_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_cols_df.append(new_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_data = pd.concat([data] + new_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_data = new_data.drop(columns=col_to_convert)\n","\n","    ### Do the same for test data\n","\n","    test_data = test_data.copy()\n","\n","    # Create a list to hold the new DataFrames\n","    new_test_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        test_data[col] = test_data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_test_col_df = pd.DataFrame(test_data[col].tolist(), index=test_data.index)\n","        new_test_col_df.columns = [f\"{col}_{idx}\" for idx in new_test_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_test_cols_df.append(new_test_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_test_data = pd.concat([test_data] + new_test_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_test_data = new_test_data.drop(columns=col_to_convert)\n","\n","\n","    ## Classify by Logistic Regression\n","\n","    X = new_data.drop(columns=['label']).values\n","    y = np.array(new_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_val = new_test_data.drop(columns=['label']).values\n","    y_val = np.array(new_test_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # make train as the whole set\n","    X_train = X\n","    y_train = y\n","\n","    sc = StandardScaler()\n","    sc.fit(X_train)\n","\n","    X_train_std = sc.transform(X_train)\n","    X_test_std = sc.transform(X_test)\n","    X_val_std = sc.transform(X_val)\n","\n","    ## hyperparameter tuning on logistic regression\n","    C_list = [0.01, 0.1, 1, 10, 100]\n","\n","    accuracy_max = 0\n","    C_max = 0\n","    lr_max = None\n","    for C in C_list:\n","        lr = LogisticRegression(C=C, random_state=42, solver='lbfgs', max_iter=500)\n","        lr.fit(X_train_std, y_train)\n","        # predict the test data\n","        y_val_pred = lr.predict(X_val_std)\n","        accuracy_C = accuracy_score(y_val, y_val_pred)\n","        if accuracy_C > accuracy_max:\n","            accuracy_max = accuracy_C\n","            C_max = C\n","            lr_max = lr\n","\n","    ## Use the best hyperparameter C\n","    #lr = LogisticRegression(C=C_max, random_state=42, solver='lbfgs', max_iter=500)\n","\n","    #lr.fit(X_train_std, y_train)\n","\n","    # predict the data\n","    y_pred = lr_max.predict(X_test_std)\n","\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'logistic regression with C_max={}'.format(C_max)\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # predict the test data\n","    y_val_pred = lr_max.predict(X_val_std)\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'logistic regression with C_max={}'.format(C_max)\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    ## Classify by Random Forest\"\"\"\n","\n","    from sklearn.ensemble import RandomForestClassifier\n","    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","    n_estimators_list = [200, 500]\n","    min_samples_split_list = [2, 10]\n","\n","    accuracy_max = 0\n","    n_estimators_max = 0\n","    min_samples_split_max = 0\n","    rf_max = None\n","    for n_estimators in n_estimators_list:\n","        for min_samples_split in min_samples_split_list:\n","            # Initialize the classifier\n","            rf_classifier = RandomForestClassifier(n_estimators=n_estimators,\n","                                                   min_samples_split=min_samples_split,\n","                                                   random_state=42)\n","\n","            # Train the classifier\n","            rf_classifier.fit(X_train, y_train)\n","            # predict the test data\n","            y_val_pred = rf_classifier.predict(X_val_std)\n","            accuracy_tuning = accuracy_score(y_val, y_val_pred)\n","            if accuracy_tuning > accuracy_max:\n","                accuracy_max = accuracy_tuning\n","                n_estimators_max = n_estimators\n","                min_samples_split_max = min_samples_split\n","                rf_max = rf_classifier\n","\n","\n","    #rf_classifier = RandomForestClassifier(n_estimators=n_estimators_max, min_samples_split =\n","                                           #min_samples_split_max, random_state=42)\n","\n","    # Train the classifier\n","    #rf_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = rf_max.predict(X_test)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_test, y_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'random forest with n_estimators={} and min_samples_split={}'.format(\n","        n_estimators_max, min_samples_split_max\n","    )\n","\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # Make predictions on individual set\n","    y_val_pred = rf_max.predict(X_val)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_val, y_val_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'random forest with n_estimators={} and min_samples_split={}'.format(\n","        n_estimators_max, min_samples_split_max\n","    )\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    ## Classify by XGBoost\"\"\"\n","\n","    from xgboost import XGBClassifier\n","\n","    n_estimators_list = [200, 500]\n","    learning_rate_list = [0.01, 0.2]\n","\n","    accuracy_max = 0\n","    n_estimators_max = 0\n","    learning_rate_max = 0\n","    xgb_max = None\n","    for n_estimators in n_estimators_list:\n","        for learning_rate in learning_rate_list:\n","            # Initialize the classifier\n","            xgb_classifier = XGBClassifier(n_estimators=n_estimators,\n","                                           learning_rate=learning_rate,\n","                                           random_state=42)\n","\n","            # Train the classifier\n","            xgb_classifier.fit(X_train, y_train)\n","            # predict the test data\n","            y_val_pred = xgb_classifier.predict(X_val_std)\n","            accuracy_tuning = accuracy_score(y_val, y_val_pred)\n","            if accuracy_tuning > accuracy_max:\n","                accuracy_max = accuracy_tuning\n","                n_estimators_max = n_estimators\n","                learning_rate_max = learning_rate\n","                xgb_max = xgb_classifier\n","\n","    # Initialize the classifier\n","    #xgb_classifier = XGBClassifier(n_estimators=n_estimators_max,\n","                                   #learning_rate=learning_rate_max,\n","                                   #random_state=42)\n","\n","    # Train the classifier\n","    #xgb_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = xgb_max.predict(X_test)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_test, y_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'XGBoost with n_estimators={} and learning_rate={}'.format(\n","        n_estimators_max, learning_rate_max\n","    )\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # Make predictions on individual set\n","    y_val_pred = xgb_max.predict(X_val)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_val, y_val_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'XGBoost with n_estimators={} and learning_rate={}'.format(\n","        n_estimators_max, learning_rate_max\n","    )\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    if len(records) % 40 == 0:\n","        evals = pd.DataFrame(records)\n","        evals.to_csv(\"test_evaluation_1_feature_results.csv\", index=None)\n","\n","evals = pd.DataFrame(records)\n","evals.to_csv(\"test_evaluation_1_feature_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6x70bAHMC18"},"outputs":[],"source":["evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga-8rQ1sKkNt"},"outputs":[],"source":["f1_boring_max = evals[evals.evaluation_data == 'individual set'].f1_boring.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnzCCnaDMC18"},"outputs":[],"source":["evals[evals.f1_boring == f1_boring_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgrq9B-cTzC5"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr94AKaPMC18"},"outputs":[],"source":["evals[evals.evaluation_data == 'individual set'].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWTaHcHATzC5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"cWh3KSQBTzC5"},"source":["## On At Least 2 Feature Combinations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMsju8hGTzC5"},"outputs":[],"source":["## on various feature combinations\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import tqdm\n","import ast\n","\n","records = []\n","\n","features_combs = generate_subsets_min_length(features_list, 2)\n","\n","for features_comb in tqdm(features_combs):\n","\n","    columns_to_use = ['label']\n","    columns_to_use.extend(features_comb)\n","\n","    data = all_data_features[columns_to_use]\n","\n","    test_data = test_data_features[columns_to_use]\n","\n","\n","    data = data.copy()\n","\n","    col_to_convert = []\n","    for acol in features_comb:\n","        if data[acol].dtype == 'object':\n","            col_to_convert.append(acol)\n","\n","    # Create a list to hold the new DataFrames\n","    new_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        data[col] = data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_col_df = pd.DataFrame(data[col].tolist(), index=data.index)\n","        new_col_df.columns = [f\"{col}_{idx}\" for idx in new_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_cols_df.append(new_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_data = pd.concat([data] + new_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_data = new_data.drop(columns=col_to_convert)\n","\n","    ### Do the same for test data\n","\n","    test_data = test_data.copy()\n","\n","    # Create a list to hold the new DataFrames\n","    new_test_cols_df = []\n","\n","    for col in col_to_convert:\n","\n","        test_data[col] = test_data[col].apply(ast.literal_eval)\n","\n","        ## Flatten the list to make individual columns for each individual elements\n","\n","        # Create a DataFrame with the new columns\n","        new_test_col_df = pd.DataFrame(test_data[col].tolist(), index=test_data.index)\n","        new_test_col_df.columns = [f\"{col}_{idx}\" for idx in new_test_col_df.columns]\n","\n","        # Append the new DataFrame to the list\n","        new_test_cols_df.append(new_test_col_df)\n","\n","    # Concatenate the original DataFrame with the new columns\n","    new_test_data = pd.concat([test_data] + new_test_cols_df, axis=1)\n","\n","    # Drop the original string columns\n","    new_test_data = new_test_data.drop(columns=col_to_convert)\n","\n","\n","    ## Classify by Logistic Regression\n","\n","    X = new_data.drop(columns=['label']).values\n","    y = np.array(new_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_val = new_test_data.drop(columns=['label']).values\n","    y_val = np.array(new_test_data['label'].map({'engaging':0, 'boring':1}))\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # make train as the whole set\n","    X_train = X\n","    y_train = y\n","\n","    sc = StandardScaler()\n","    sc.fit(X_train)\n","\n","    X_train_std = sc.transform(X_train)\n","    X_test_std = sc.transform(X_test)\n","    X_val_std = sc.transform(X_val)\n","\n","    ## hyperparameter tuning on logistic regression\n","    C_list = [0.01, 0.1, 1, 10, 100]\n","\n","    accuracy_max = 0\n","    C_max = 0\n","    lr_max = None\n","    for C in C_list:\n","        lr = LogisticRegression(C=C, random_state=42, solver='lbfgs', max_iter=500)\n","        lr.fit(X_train_std, y_train)\n","        # predict the test data\n","        y_val_pred = lr.predict(X_val_std)\n","        accuracy_C = accuracy_score(y_val, y_val_pred)\n","        if accuracy_C > accuracy_max:\n","            accuracy_max = accuracy_C\n","            C_max = C\n","            lr_max = lr\n","\n","    ## Use the best hyperparameter C\n","    #lr = LogisticRegression(C=C_max, random_state=42, solver='lbfgs', max_iter=500)\n","\n","    #lr.fit(X_train_std, y_train)\n","\n","    # predict the data\n","    y_pred = lr_max.predict(X_test_std)\n","\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'logistic regression with C_max={}'.format(C_max)\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # predict the test data\n","    y_val_pred = lr_max.predict(X_val_std)\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'logistic regression with C_max={}'.format(C_max)\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    ## Classify by Random Forest\"\"\"\n","\n","    from sklearn.ensemble import RandomForestClassifier\n","    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","    n_estimators_list = [200, 500]\n","    min_samples_split_list = [2, 10]\n","\n","    accuracy_max = 0\n","    n_estimators_max = 0\n","    min_samples_split_max = 0\n","    rf_max = None\n","    for n_estimators in n_estimators_list:\n","        for min_samples_split in min_samples_split_list:\n","            # Initialize the classifier\n","            rf_classifier = RandomForestClassifier(n_estimators=n_estimators,\n","                                                   min_samples_split=min_samples_split,\n","                                                   random_state=42)\n","\n","            # Train the classifier\n","            rf_classifier.fit(X_train, y_train)\n","            # predict the test data\n","            y_val_pred = rf_classifier.predict(X_val_std)\n","            accuracy_tuning = accuracy_score(y_val, y_val_pred)\n","            if accuracy_tuning > accuracy_max:\n","                accuracy_max = accuracy_tuning\n","                n_estimators_max = n_estimators\n","                min_samples_split_max = min_samples_split\n","                rf_max = rf_classifier\n","\n","\n","    #rf_classifier = RandomForestClassifier(n_estimators=n_estimators_max, min_samples_split =\n","    #                                      min_samples_split_max, random_state=42)\n","\n","    # Train the classifier\n","    #rf_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = rf_max.predict(X_test)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_test, y_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'random forest with n_estimators={} and min_samples_split={}'.format(\n","        n_estimators_max, min_samples_split_max\n","    )\n","\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # Make predictions on individual set\n","    y_val_pred = rf_max.predict(X_val)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_val, y_val_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'random forest with n_estimators={} and min_samples_split={}'.format(\n","        n_estimators_max, min_samples_split_max\n","    )\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    ## Classify by XGBoost\"\"\"\n","\n","    from xgboost import XGBClassifier\n","\n","    n_estimators_list = [200, 500]\n","    learning_rate_list = [0.01, 0.2]\n","\n","    accuracy_max = 0\n","    n_estimators_max = 0\n","    learning_rate_max = 0\n","    xgb_max = None\n","    for n_estimators in n_estimators_list:\n","        for learning_rate in learning_rate_list:\n","            # Initialize the classifier\n","            xgb_classifier = XGBClassifier(n_estimators=n_estimators,\n","                                           learning_rate=learning_rate,\n","                                           random_state=42)\n","\n","            # Train the classifier\n","            xgb_classifier.fit(X_train, y_train)\n","            # predict the test data\n","            y_val_pred = xgb_classifier.predict(X_val_std)\n","            accuracy_tuning = accuracy_score(y_val, y_val_pred)\n","            if accuracy_tuning > accuracy_max:\n","                accuracy_max = accuracy_tuning\n","                n_estimators_max = n_estimators\n","                learning_rate_max = learning_rate\n","                xgb_max = xgb_classifier\n","\n","    # Initialize the classifier\n","    #xgb_classifier = XGBClassifier(n_estimators=n_estimators_max,\n","    #                               learning_rate=learning_rate_max,\n","    #                               random_state=42)\n","\n","    # Train the classifier\n","    #xgb_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = xgb_max.predict(X_test)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_test, y_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_test, y_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_test, y_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'XGBoost with n_estimators={} and learning_rate={}'.format(\n","        n_estimators_max, learning_rate_max\n","    )\n","    rec['evaluation_data'] = 'split from training'\n","    rec['accuracy'] = accuracy_score(y_test, y_pred)\n","    rec['precision_boring'] = precision_score(y_test, y_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_test, y_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_test, y_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_test, y_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_test, y_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_test, y_pred, pos_label=0)\n","    records.append(rec)\n","\n","    # Make predictions on individual set\n","    y_val_pred = xgb_max.predict(X_val)\n","\n","    # Evaluate the predictions\n","    #print(\"Confusion Matrix:\")\n","    #print(confusion_matrix(y_val, y_val_pred))\n","\n","    #print(\"\\nClassification Report:\")\n","    #print(classification_report(y_val, y_val_pred))\n","\n","    #print(\"\\nAccuracy Score:\")\n","    #print(accuracy_score(y_val, y_val_pred))\n","\n","    rec = {}\n","    rec['features'] = features_comb\n","    rec['model'] = 'XGBoost with n_estimators={} and learning_rate={}'.format(\n","        n_estimators_max, learning_rate_max\n","    )\n","    rec['evaluation_data'] = 'individual set'\n","    rec['accuracy'] = accuracy_score(y_val, y_val_pred)\n","    rec['precision_boring'] = precision_score(y_val, y_val_pred, pos_label=1)\n","    rec['recall_boring'] = recall_score(y_val, y_val_pred, pos_label=1)\n","    rec['f1_boring'] = f1_score(y_val, y_val_pred, pos_label=1)\n","    rec['precision_engaging'] = precision_score(y_val, y_val_pred, pos_label=0)\n","    rec['recall_engaging'] = recall_score(y_val, y_val_pred, pos_label=0)\n","    rec['f1_engaging'] = f1_score(y_val, y_val_pred, pos_label=0)\n","    records.append(rec)\n","\n","    if len(records) % 40 == 0:\n","        evals = pd.DataFrame(records)\n","        evals.to_csv(\"test_evaluation_2_features_results.csv\", index=None)\n","\n","evals = pd.DataFrame(records)\n","evals.to_csv(\"test_evaluation_2_features_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4UPzUhUTzC5"},"outputs":[],"source":["evals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YyR3HJJTzC6"},"outputs":[],"source":["f1_boring_max = evals[evals.evaluation_data == 'individual set'].f1_boring.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1giuHlNyTzC6"},"outputs":[],"source":["evals[evals.f1_boring == f1_boring_max]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82eIFnTJTzC6"},"outputs":[],"source":["evals[evals.evaluation_data == 'individual set'].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0LYwurhTzC6"},"outputs":[],"source":["evals[(evals.evaluation_data == 'individual set') & (evals.model.str.contains(\"random forest\"))\n","                                                       ].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6ywNYstMC18"},"outputs":[],"source":["data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FIBMQClMC18"},"outputs":[],"source":["data[['zcrate_mean', 'chroma_cqt_mean', 'spcent_mean', 'spband_mean']]"]},{"cell_type":"markdown","metadata":{"id":"B29jj0owMC18"},"source":["## Combine 1-feature and 2-features Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlkelcnHTzC6"},"outputs":[],"source":["test_1_feature_results = pd.read_csv(\"test_evaluation_1_feature_results.csv\")\n","test_1_feature_results.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YU2xNeHTzC6"},"outputs":[],"source":["test_2_features_results = pd.read_csv(\"test_evaluation_2_features_results.csv\")\n","test_2_features_results.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31Ylp3IdTzC6"},"outputs":[],"source":["test_all_results = pd.concat([test_1_feature_results, test_2_features_results], ignore_index=True)\n","test_all_results.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruGukS0UTzC_"},"outputs":[],"source":["test_all_results.to_csv(\"test_evaluation_all_results.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAbkTNs9TzC_"},"outputs":[],"source":["evals[(evals.evaluation_data == 'individual set')].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ur126vWTzC_"},"outputs":[],"source":["evals[(evals.evaluation_data == 'individual set') & (evals.model.str.contains(\"random forest\"))\n","                                                       ].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAs9zuI_TzC_"},"outputs":[],"source":["evals[(evals.evaluation_data == 'individual set') & (evals.model.str.contains(\"XGBoost\"))\n","                                                       ].sort_values(by='f1_boring', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hrnapfvTzC_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"drtutor","language":"python","name":"drtutor"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}